# å¿«é€Ÿä¿®å¤ç‰ˆæœ¬3ï¼šæ”¹è¿›è¶…å‚æ•°ä¼˜åŒ–
# æ›¿æ¢åŸä»£ç ä¸­çš„è¶…å‚æ•°ä¼˜åŒ–éƒ¨åˆ†

# ========== æ”¹è¿›çš„è¶…å‚æ•°ä¼˜åŒ–é…ç½® ==========

# æ–¹æ¡ˆ1ï¼šé€‚ä¸­çš„ä¼˜åŒ–ï¼ˆæ¨èç”¨äºå¿«é€Ÿå®éªŒï¼‰
if OPTUNA_AVAILABLE:
    print("âš¡ ä½¿ç”¨æ”¹è¿›çš„è¶…å‚æ•°å¯»ä¼˜...")
    optimizer = SimpleHyperparameterOptimizer(
        train_dataset, val_dataset, 
        n_trials=10,              # 2 â†’ 10
        ultra_fast_mode=False     # å…³é—­æé€Ÿæ¨¡å¼
    )
    optimizer.ultra_fast_epochs = 30  # 5 â†’ 30
    optimizer.ultra_fast_eval_samples = 10  # 3 â†’ 10
    best_params = optimizer.optimize()
else:
    # æ”¹è¿›çš„é»˜è®¤å‚æ•°
    best_params = {
        'hidden_dim': 256,
        'z_dim': 64,
        'lr_G': 2e-4,
        'lr_D': 1e-4,
        'batch_size': 32,        # 64 â†’ 32ï¼ˆæé«˜ç¨³å®šæ€§ï¼‰
        'n_critic': 3,           # 5 â†’ 3ï¼ˆåŠ å¿«Gæ›´æ–°ï¼‰
        'lambda_gp': 10,
        'noise_sigma': 0.2,
        # å¹³è¡¡çš„æŸå¤±æƒé‡
        'div_loss_weight': 1.5,      # 0.3 â†’ 1.5
        'intra_div_weight': 2.0,     # 0.4 â†’ 2.0
        'temporal_weight': 0.3,      # 0.5 â†’ 0.3
        'autocorr_weight': 0.5,      # 0.5 â†’ 0.5
        'freq_weight': 0.2,          # 0.4 â†’ 0.2
    }


# ========== æ”¹è¿›çš„è®­ç»ƒå‚æ•° ==========
epochs = 1000              # 2000 â†’ 1000ï¼ˆå…ˆçœ‹æ•ˆæœï¼‰
batch_size = 32            # 64 â†’ 32
n_critic = 3               # 5 â†’ 3
early_stopping_patience = 100  # 50 â†’ 100


# ========== åŠ¨æ€å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥ ==========
def dynamic_lr_adjustment(opt_G, opt_D, avg_wasserstein, epoch, 
                         w_threshold=0.3, adjustment_factor=0.9):
    """
    åŠ¨æ€å­¦ä¹ ç‡è°ƒæ•´
    
    å½“Wè·ç¦»è¿‡å¤§æ—¶ï¼Œé™ä½å­¦ä¹ ç‡ä»¥æé«˜ç¨³å®šæ€§
    """
    if epoch > 100 and avg_wasserstein > w_threshold:
        for param_group in opt_G.param_groups:
            param_group['lr'] *= adjustment_factor
        for param_group in opt_D.param_groups:
            param_group['lr'] *= adjustment_factor
        
        print(f"  ğŸ“‰ åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡: G_lr={opt_G.param_groups[0]['lr']:.6f}, "
              f"D_lr={opt_D.param_groups[0]['lr']:.6f}")
        
        return True
    return False


# ========== æ”¹è¿›çš„æ—©åœç­–ç•¥ ==========
class ImprovedEarlyStopping:
    """æ”¹è¿›çš„æ—©åœç­–ç•¥ï¼šåŒæ—¶ç›‘æ§Wè·ç¦»å’ŒMAPE"""
    
    def __init__(self, patience=100, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_wasserstein = float('inf')
        self.best_mape = float('inf')
        self.should_stop = False
    
    def __call__(self, avg_wasserstein, avg_mape=None):
        # ç»¼åˆè¯„åˆ†ï¼šWè·ç¦»ä¸ºä¸»ï¼ŒMAPEä¸ºè¾…
        if avg_mape is not None:
            score = avg_wasserstein + 0.01 * avg_mape  # MAPEæƒé‡è¾ƒå°
        else:
            score = avg_wasserstein
        
        # æ£€æŸ¥æ˜¯å¦æ”¹å–„
        if score < self.best_wasserstein - self.min_delta:
            self.best_wasserstein = score
            self.counter = 0
            return False
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.should_stop = True
                print(f"  ğŸ›‘ æ—©åœè§¦å‘! è¿ç»­{self.patience}è½®æ— æ”¹å–„")
                print(f"  ğŸ“Š æœ€ä½³Wè·ç¦»: {self.best_wasserstein:.4f}")
                return True
        
        return False


# ========== ä½¿ç”¨æ–¹æ³• ==========
"""
# 1. åœ¨è®­ç»ƒå‰åˆ›å»ºæ—©åœå¯¹è±¡ï¼š
early_stopping = ImprovedEarlyStopping(patience=100, min_delta=0.001)

# 2. åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨ï¼š
for epoch in range(1, epochs + 1):
    # ... è®­ç»ƒä»£ç  ...
    
    if epoch % 20 == 0:
        avg_loss_G = epoch_loss_G / batch_count
        avg_loss_D = epoch_loss_D / batch_count
        avg_wasserstein = epoch_wasserstein / batch_count
        
        # åŠ¨æ€å­¦ä¹ ç‡è°ƒæ•´
        dynamic_lr_adjustment(opt_G, opt_D, avg_wasserstein, epoch)
        
        # æ—©åœæ£€æŸ¥
        if early_stopping(avg_wasserstein):
            print("è®­ç»ƒæå‰ç»“æŸï¼ˆæ—©åœæœºåˆ¶ï¼‰")
            break
"""


# ========== æ”¹è¿›çš„è¶…å‚æ•°ä¼˜åŒ–ç›®æ ‡å‡½æ•° ==========
def improved_objective_function(trial, train_dataset, val_dataset):
    """
    æ”¹è¿›çš„è¶…å‚æ•°ä¼˜åŒ–ç›®æ ‡å‡½æ•°
    
    å…³é”®æ”¹è¿›ï¼š
    1. å¢åŠ è®­ç»ƒè½®æ•°ï¼ˆ5 â†’ 30ï¼‰
    2. å¢åŠ è¯„ä¼°æ ·æœ¬æ•°ï¼ˆ3 â†’ 10ï¼‰
    3. ä½¿ç”¨ç»¼åˆè¯„åˆ†ï¼ˆWè·ç¦» + å¤šæ ·æ€§ï¼‰
    """
    # è¶…å‚æ•°ç©ºé—´
    params = {
        'hidden_dim': trial.suggest_categorical('hidden_dim', [128, 256, 512]),
        'z_dim': trial.suggest_categorical('z_dim', [32, 64, 100]),
        'lr_G': trial.suggest_float('lr_G', 1e-5, 5e-4, log=True),
        'lr_D': trial.suggest_float('lr_D', 1e-5, 5e-4, log=True),
        'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64]),
        'n_critic': trial.suggest_categorical('n_critic', [1, 3, 5]),
        'lambda_gp': trial.suggest_float('lambda_gp', 5, 15),
        'noise_sigma': trial.suggest_float('noise_sigma', 0.1, 0.3),
    }
    
    # è®­ç»ƒé…ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    G = Generator(z_dim=params['z_dim'], hidden=params['hidden_dim']).to(device)
    D = Discriminator(hidden=params['hidden_dim']).to(device)
    
    opt_G = optim.Adam(G.parameters(), lr=params['lr_G'], betas=(0.5, 0.9))
    opt_D = optim.Adam(D.parameters(), lr=params['lr_D'], betas=(0.5, 0.9))
    
    dataloader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)
    
    # è®­ç»ƒ30è½®ï¼ˆè€Œä¸æ˜¯5è½®ï¼‰
    for epoch in range(30):
        for batch_idx, (real_curves, forecast_curves, season_vec) in enumerate(dataloader):
            if batch_idx >= 10:  # æ¯è½®æœ€å¤š10ä¸ªbatch
                break
            
            real_curves = real_curves.to(device)
            forecast_curves = forecast_curves.to(device)
            season_vec = season_vec.to(device)
            batch_size = real_curves.size(0)
            
            # æ›´æ–°åˆ¤åˆ«å™¨
            for _ in range(params['n_critic']):
                z = torch.randn(batch_size, params['z_dim'], device=device) * params['noise_sigma']
                fake_curves = G(z, season_vec, forecast_curves, 0).detach()
                d_real, aux_real = D(real_curves, season_vec, forecast_curves)
                d_fake, aux_fake = D(fake_curves, season_vec, forecast_curves)
                
                loss_D = d_fake.mean() - d_real.mean()
                gp = gradient_penalty(D, real_curves, fake_curves, season_vec, forecast_curves)
                aux_loss = F.cross_entropy(aux_real, season_vec.argmax(dim=1))
                
                opt_D.zero_grad()
                (loss_D + params['lambda_gp'] * gp + aux_loss).backward()
                opt_D.step()
            
            # æ›´æ–°ç”Ÿæˆå™¨
            z = torch.randn(batch_size, params['z_dim'], device=device) * params['noise_sigma']
            gen_curves = G(z, season_vec, forecast_curves, 0)
            d_gen, aux_gen = D(gen_curves, season_vec, forecast_curves)
            
            loss_G = -d_gen.mean()
            aux_loss_G = F.cross_entropy(aux_gen, season_vec.argmax(dim=1))
            
            # ç®€åŒ–çš„æŸå¤±ï¼ˆå¿«é€Ÿä¼˜åŒ–ï¼‰
            mse_loss = F.mse_loss(gen_curves, real_curves)
            total_loss_G = loss_G + aux_loss_G + 2.0 * mse_loss
            
            opt_G.zero_grad()
            total_loss_G.backward()
            opt_G.step()
    
    # è¯„ä¼°ï¼ˆç”Ÿæˆ10ä¸ªæ ·æœ¬è€Œä¸æ˜¯3ä¸ªï¼‰
    G.eval()
    with torch.no_grad():
        z = torch.randn(40, params['z_dim'], device=device) * params['noise_sigma']
        season_vec = torch.eye(4, device=device).repeat(10, 1)
        
        # ä½¿ç”¨éªŒè¯é›†çš„é¢„æµ‹æ•°æ®
        forecast_samples = []
        for _ in range(10):
            random_idx = torch.randint(0, len(val_dataset), (4,))
            forecast_samples.append(val_dataset.forecast_curves[random_idx])
        forecast_all = torch.cat(forecast_samples, dim=0).to(device)
        
        fake_curves = G(z, season_vec, forecast_all, 0).detach().cpu().numpy()
        
        # è®¡ç®—å¤šæ ·æ€§è¯„åˆ†
        diversity_score = 0
        for season in range(4):
            season_curves = fake_curves[season*10:(season+1)*10]
            if len(season_curves) > 1:
                diff_matrix = np.abs(season_curves[:, np.newaxis, :] - season_curves[np.newaxis, :, :])
                upper_tri_mask = np.triu(np.ones((len(season_curves), len(season_curves))), k=1).astype(bool)
                diversity_score += np.mean(diff_matrix[upper_tri_mask])
        
        diversity_score /= 4
    
    # æ¸…ç†GPUå†…å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    return diversity_score  # ç›®æ ‡ï¼šæœ€å¤§åŒ–å¤šæ ·æ€§


# ========== å®Œæ•´çš„è¶…å‚æ•°ä¼˜åŒ–ç¤ºä¾‹ ==========
"""
if OPTUNA_AVAILABLE:
    study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))
    study.optimize(
        lambda trial: improved_objective_function(trial, train_dataset, val_dataset),
        n_trials=10,
        show_progress_bar=True
    )
    
    print(f"âœ… è¶…å‚æ•°å¯»ä¼˜å®Œæˆï¼æœ€ä½³åˆ†æ•°: {study.best_value:.4f}")
    best_params = study.best_params
    
    # æ·»åŠ å›ºå®šå‚æ•°
    best_params.update({
        'lambda_gp': 10,
        'noise_sigma': 0.2,
        'div_loss_weight': 1.5,
        'intra_div_weight': 2.0,
        'temporal_weight': 0.3,
        'autocorr_weight': 0.5,
        'freq_weight': 0.2,
    })
"""
