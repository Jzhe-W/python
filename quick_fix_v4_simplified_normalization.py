# å¿«é€Ÿä¿®å¤ç‰ˆæœ¬4ï¼šç®€åŒ–å½’ä¸€åŒ–æ–¹æ¡ˆ
# æ›¿æ¢åŸä»£ç ä¸­çš„åŒé‡å½’ä¸€åŒ–ï¼ˆZ-score + MinMaxï¼‰ä¸ºå•ä¸€MinMax

# ========== ç®€åŒ–çš„å½’ä¸€åŒ–æ–¹æ¡ˆ ==========

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler

def simplified_normalization(data, forecast_data):
    """
    ç®€åŒ–çš„å½’ä¸€åŒ–æ–¹æ¡ˆï¼šåªä½¿ç”¨MinMaxå½’ä¸€åŒ–
    
    åŸé—®é¢˜ï¼š
    1. Z-score + MinMaxåŒé‡å½’ä¸€åŒ–å¯¼è‡´ä¿¡æ¯æŸå¤±
    2. å­£èŠ‚ç‰¹å®šçš„Z-scoreå‚æ•°å¤æ‚ä¸”å®¹æ˜“å‡ºé”™
    3. åå½’ä¸€åŒ–æ—¶ç´¢å¼•é”™è¯¯
    
    æ”¹è¿›æ–¹æ¡ˆï¼š
    - åªä½¿ç”¨MinMaxå½’ä¸€åŒ–åˆ°[0, 1]
    - ç®€åŒ–åå½’ä¸€åŒ–æµç¨‹
    - é¿å…å­£èŠ‚ç´¢å¼•é”™è¯¯
    """
    print("ğŸ”„ ç®€åŒ–å½’ä¸€åŒ–æ–¹æ¡ˆï¼ˆåªä½¿ç”¨MinMaxï¼‰...")
    
    # 1. ç›´æ¥ä½¿ç”¨MinMaxå½’ä¸€åŒ–
    scaler = MinMaxScaler(feature_range=(0, 1))
    
    # å½’ä¸€åŒ–çœŸå®æ•°æ®
    data_norm = pd.DataFrame(
        scaler.fit_transform(data.values),
        index=data.index,
        columns=data.columns
    )
    
    # å½’ä¸€åŒ–é¢„æµ‹æ•°æ®ï¼ˆä½¿ç”¨ç›¸åŒçš„scalerï¼‰
    forecast_data_norm = pd.DataFrame(
        scaler.transform(forecast_data.values),
        index=forecast_data.index,
        columns=forecast_data.columns
    )
    
    print(f"âœ… å½’ä¸€åŒ–å®Œæˆ")
    print(f"  çœŸå®æ•°æ®èŒƒå›´: [{data_norm.min().min():.4f}, {data_norm.max().max():.4f}]")
    print(f"  é¢„æµ‹æ•°æ®èŒƒå›´: [{forecast_data_norm.min().min():.4f}, {forecast_data_norm.max().max():.4f}]")
    
    return data_norm, forecast_data_norm, scaler


def simplified_denormalization(fake_curves, scaler):
    """
    ç®€åŒ–çš„åå½’ä¸€åŒ–
    
    è¾“å…¥ï¼šfake_curves [4, 96] - ç”Ÿæˆçš„4ä¸ªå­£èŠ‚æ ·æœ¬
    è¾“å‡ºï¼šfake_denorm [4, 96] - åå½’ä¸€åŒ–åçš„æ ·æœ¬
    """
    # è½¬ç½®ä¸º (96, 4) è¿›è¡Œæ‰¹é‡å¤„ç†
    fake_reshaped = fake_curves.T  # (96, 4)
    
    # åå½’ä¸€åŒ–
    fake_denorm = scaler.inverse_transform(fake_reshaped)
    
    # è½¬ç½®å› (4, 96)
    fake_denorm = fake_denorm.T
    
    # ç¡®ä¿é£ç”µå‡ºåŠ›éè´Ÿ
    fake_denorm = np.clip(fake_denorm, 0, None)
    
    print(f"âœ… åå½’ä¸€åŒ–å®Œæˆï¼ŒèŒƒå›´: [{fake_denorm.min():.2f}, {fake_denorm.max():.2f}]")
    
    return fake_denorm


# ========== ä½¿ç”¨æ–¹æ³• ==========
"""
# æ›¿æ¢åŸä»£ç ä¸­çš„å½’ä¸€åŒ–éƒ¨åˆ†ï¼ˆçº¦ç¬¬100-200è¡Œï¼‰ï¼š

# åŸä»£ç ï¼š
# å­£èŠ‚ç‰¹å®šçš„å½’ä¸€åŒ–ç­–ç•¥
data_zscore = data.copy()
for i, season in enumerate(season_names):
    season_indices = season_indices_dict[i]
    season_mean = season_stats[season]['mean']
    season_std = season_stats[season]['std']
    data_zscore.iloc[:, season_indices] = (data.iloc[:, season_indices] - season_mean) / (season_std + 1e-8)

scaler = MinMaxScaler(feature_range=(0, 1))
data_norm = pd.DataFrame(scaler.fit_transform(data_zscore.values), ...)

# æ›¿æ¢ä¸ºï¼š
data_norm, forecast_data_norm, scaler = simplified_normalization(data, forecast_data)


# æ›¿æ¢åŸä»£ç ä¸­çš„åå½’ä¸€åŒ–éƒ¨åˆ†ï¼ˆçº¦ç¬¬1600è¡Œï¼‰ï¼š

# åŸä»£ç ï¼š
fake_reshaped = fake_all.T
for i, season in enumerate(season_names):
    season_fake = fake_reshaped[:, i]
    season_indices = season_indices_dict[i]
    season_zscore_min = zscore_min[season_indices].mean()
    season_zscore_max = zscore_max[season_indices].mean()
    ...

# æ›¿æ¢ä¸ºï¼š
fake_denorm = simplified_denormalization(fake_all, scaler)
"""


# ========== å®Œæ•´çš„ç®€åŒ–æµç¨‹ç¤ºä¾‹ ==========
def complete_simplified_pipeline():
    """å®Œæ•´çš„ç®€åŒ–æ•°æ®å¤„ç†æµç¨‹"""
    
    # 1. è¯»å–æ•°æ®
    data = pd.read_excel('data_2024.07.01-2025.07.01.xlsm')
    forecast_data = pd.read_excel('data_2024.07.01-2025.07.01_forecast.xlsm')
    
    # 2. ç®€åŒ–å½’ä¸€åŒ–
    data_norm, forecast_data_norm, scaler = simplified_normalization(data, forecast_data)
    
    # 3. æ„é€ å­£èŠ‚æ ‡ç­¾ï¼ˆä¿æŒä¸å˜ï¼‰
    days = data_norm.columns.tolist()
    season_labels = np.zeros((len(days), 4), dtype=np.float32)
    indices = np.arange(1, len(days) + 1)
    
    summer_mask = (indices >= 1) & (indices <= 62) | (indices >= 336) & (indices <= 365)
    autumn_mask = (indices >= 63) & (indices <= 153)
    winter_mask = (indices >= 154) & (indices <= 243)
    spring_mask = (indices >= 244) & (indices <= 335)
    
    season_labels[summer_mask, 1] = 1
    season_labels[autumn_mask, 2] = 1
    season_labels[winter_mask, 3] = 1
    season_labels[spring_mask, 0] = 1
    
    # 4. æ•°æ®åˆ†å‰²ï¼ˆä¿æŒä¸å˜ï¼‰
    season_indices = {s: np.where(season_labels[:, s] == 1)[0] for s in range(4)}
    
    real_train_idx = []
    real_val_idx = []
    for s in range(4):
        indices = season_indices[s]
        perm = np.random.permutation(indices)
        val_count = int(len(indices) * 0.2)
        real_val_idx.extend(perm[:val_count])
        real_train_idx.extend(perm[val_count:])
    
    real_train_idx = np.array(real_train_idx)
    real_val_idx = np.array(real_val_idx)
    
    # 5. æ„é€ æ•°æ®é›†
    real_train_data = data_norm.iloc[:, real_train_idx]
    real_train_labels = season_labels[real_train_idx]
    
    forecast_train_data = forecast_data_norm.iloc[:, real_train_idx]
    forecast_train_labels = season_labels[real_train_idx]
    
    # 6. åˆ›å»ºDatasetï¼ˆä¿æŒä¸å˜ï¼‰
    train_dataset = ForecastIntegratedDataset(
        real_train_data, 
        forecast_train_data, 
        real_train_labels
    )
    
    # 7. è®­ç»ƒæ¨¡å‹ï¼ˆä¿æŒä¸å˜ï¼‰
    # ... è®­ç»ƒä»£ç  ...
    
    # 8. ç”Ÿæˆæ ·æœ¬
    G.eval()
    with torch.no_grad():
        z_all = torch.randn(4, z_dim, device=device) * noise_sigma
        season_eye = torch.eye(4, device=device)
        
        if USE_CONDITIONAL_GAN:
            forecast_samples = []
            for season in range(4):
                random_idx = torch.randint(0, len(val_dataset), (1,))
                forecast_samples.append(val_dataset.forecast_curves[random_idx])
            forecast_all = torch.cat(forecast_samples, dim=0).to(device)
            fake_all = G(z_all, season_eye, forecast_all, 0).detach().cpu().numpy()
        else:
            fake_all = G(z_all, season_eye, 0).detach().cpu().numpy()
    
    # 9. ç®€åŒ–åå½’ä¸€åŒ–
    fake_denorm = simplified_denormalization(fake_all, scaler)
    
    # 10. å¯¹æ¯”åˆ†æï¼ˆä¿æŒä¸å˜ï¼‰
    # ... å¯¹æ¯”ä»£ç  ...
    
    return fake_denorm


# ========== é¢å¤–çš„æ•°æ®è´¨é‡æ£€æŸ¥ ==========
def validate_normalization(data_norm, data_original, scaler):
    """éªŒè¯å½’ä¸€åŒ–çš„æ­£ç¡®æ€§"""
    print("\nğŸ” å½’ä¸€åŒ–éªŒè¯:")
    
    # 1. æ£€æŸ¥å½’ä¸€åŒ–èŒƒå›´
    assert data_norm.min().min() >= 0, "å½’ä¸€åŒ–åæœ€å°å€¼ < 0"
    assert data_norm.max().max() <= 1, "å½’ä¸€åŒ–åæœ€å¤§å€¼ > 1"
    print("âœ… å½’ä¸€åŒ–èŒƒå›´æ­£ç¡®: [0, 1]")
    
    # 2. æ£€æŸ¥åå½’ä¸€åŒ–
    data_denorm = scaler.inverse_transform(data_norm.values)
    reconstruction_error = np.abs(data_denorm - data_original.values).mean()
    print(f"âœ… åå½’ä¸€åŒ–é‡æ„è¯¯å·®: {reconstruction_error:.6f}")
    
    if reconstruction_error > 1e-6:
        print("âš ï¸  è­¦å‘Šï¼šé‡æ„è¯¯å·®è¾ƒå¤§ï¼Œå¯èƒ½å­˜åœ¨æ•°å€¼ç²¾åº¦é—®é¢˜")
    
    # 3. æ£€æŸ¥æ•°æ®åˆ†å¸ƒ
    print(f"  åŸå§‹æ•°æ®: å‡å€¼={data_original.values.mean():.2f}, "
          f"æ ‡å‡†å·®={data_original.values.std():.2f}")
    print(f"  å½’ä¸€åŒ–æ•°æ®: å‡å€¼={data_norm.values.mean():.4f}, "
          f"æ ‡å‡†å·®={data_norm.values.std():.4f}")
    print(f"  é‡æ„æ•°æ®: å‡å€¼={data_denorm.mean():.2f}, "
          f"æ ‡å‡†å·®={data_denorm.std():.2f}")
    
    return reconstruction_error < 1e-6
